{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (from optuna) (1.13.2)\n",
      "Requirement already satisfied: colorlog in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (from optuna) (6.8.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (from optuna) (23.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (from optuna) (2.0.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (from optuna) (4.65.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.9.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: swig in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (4.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gymnasium[box2d] in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (4.9.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (2.6.0)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\terry teh\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (4.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install optuna\n",
    "%pip install swig\n",
    "%pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNTrainer:\n",
    "    def __init__(self, env, main_network, target_network, optimizer, replay_buffer, model_path='./model/model.pth', gamma=0.99, batch_size=64, target_update_frequency=1000):\n",
    "        self.env = env\n",
    "        self.main_network = main_network\n",
    "        self.target_network = target_network\n",
    "        self.optimizer = optimizer\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.model_path = model_path\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_frequency = target_update_frequency\n",
    "        self.step_count = 0\n",
    "\n",
    "        # Load the model if it exists\n",
    "        if os.path.exists(os.path.dirname(self.model_path)):\n",
    "            if os.path.isfile(self.model_path):\n",
    "                self.main_network.load_state_dict(torch.load(self.model_path))\n",
    "                self.target_network.load_state_dict(torch.load(self.model_path))\n",
    "                print(\"Loaded model from disk\")\n",
    "        else:\n",
    "            os.makedirs(os.path.dirname(self.model_path))\n",
    "\n",
    "    def train(self, num_episodes, save=True):\n",
    "        total_rewards = []\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()  # Extract the state from the returned tuple\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                self.env.render()  # Add this line to render the environment\n",
    "                # Ensure the state is in the correct shape by adding an extra dimension\n",
    "                action = self.main_network(torch.FloatTensor(state).unsqueeze(0)).argmax(dim=1).item()\n",
    "                next_state, reward, done, _, _ = self.env.step(action)  # Extract the next_state from the returned tuple\n",
    "                self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                if len(self.replay_buffer) >= self.batch_size:\n",
    "                    self.update_network()\n",
    "\n",
    "            total_rewards.append(total_reward)\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "        # Save the model after training\n",
    "        if save:\n",
    "            torch.save(self.main_network.state_dict(), self.model_path)\n",
    "            print(\"Saved model to disk\")\n",
    "\n",
    "        self.env.close()\n",
    "        return sum(total_rewards) / len(total_rewards)  # Return average reward\n",
    "\n",
    "    def update_network(self):\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = torch.FloatTensor(state_batch)\n",
    "        action_batch = torch.LongTensor(action_batch)\n",
    "        reward_batch = torch.FloatTensor(reward_batch)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch)\n",
    "        done_batch = torch.FloatTensor(done_batch)\n",
    "\n",
    "        # Calculate the current Q-values\n",
    "        q_values = self.main_network(state_batch).gather(1, action_batch.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Calculate the target Q-values\n",
    "        next_q_values = self.target_network(next_state_batch).max(1)[0]\n",
    "        expected_q_values = reward_batch + self.gamma * next_q_values * (1 - done_batch)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = nn.MSELoss()(q_values, expected_q_values.detach())\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Periodically update the target network\n",
    "        if self.step_count % self.target_update_frequency == 0:\n",
    "            self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "\n",
    "        self.step_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, env, main_network, target_network, replay_buffer, model_path, params_path='./model/params.pkl'):\n",
    "        self.env = env\n",
    "        self.main_network = main_network\n",
    "        self.target_network = target_network\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.model_path = model_path\n",
    "        self.params_path = params_path\n",
    "\n",
    "    def objective(self, trial, n_episodes=10):\n",
    "        lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "        gamma = trial.suggest_uniform('gamma', 0.9, 0.999)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "        target_update_frequency = trial.suggest_categorical('target_update_frequency', [500, 1000, 2000])\n",
    "\n",
    "        optimizer = optim.Adam(self.main_network.parameters(), lr=lr)\n",
    "        trainer = DQNTrainer(self.env, self.main_network, self.target_network, optimizer, self.replay_buffer, self.model_path, gamma=gamma, batch_size=batch_size, target_update_frequency=target_update_frequency)\n",
    "        reward = trainer.train(n_episodes, save=False)\n",
    "        return reward\n",
    "\n",
    "    def optimize(self, n_trials=100, save_params=True):\n",
    "        if not TRAIN and os.path.isfile(self.params_path):\n",
    "            with open(self.params_path, 'rb') as f:\n",
    "                best_params = pickle.load(f)\n",
    "            print(\"Loaded parameters from disk\")\n",
    "        elif not FINETUNE:\n",
    "            best_params = {\n",
    "                'lr': LEARNING_RATE, \n",
    "                'gamma': GAMMA, \n",
    "                'batch_size': BATCH_SIZE, \n",
    "                'target_update_frequency': TARGET_UPDATE_FREQUENCY\n",
    "                }\n",
    "            print(f\"Using default parameters: {best_params}\")\n",
    "        else:\n",
    "            print(\"Optimizing hyperparameters\")\n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(self.objective, n_trials=n_trials)\n",
    "            best_params = study.best_params\n",
    "\n",
    "            if save_params:\n",
    "                with open(self.params_path, 'wb') as f:\n",
    "                    pickle.dump(best_params, f)\n",
    "                print(\"Saved parameters to disk\")\n",
    "\n",
    "        return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "FINETUNE = True\n",
    "\n",
    "# Set the following hyperparameters if FINETUNE is False\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE_FREQUENCY = 1000\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_network = DQN(state_dim, action_dim)\n",
    "target_network = DQN(state_dim, action_dim)\n",
    "target_network.load_state_dict(main_network.state_dict())\n",
    "target_network.eval()\n",
    "\n",
    "replay_buffer = ReplayBuffer(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-11 00:18:51,392] A new study created in memory with name: no-name-b0bf83e3-e24a-4035-a5b0-b2f305baffaf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Terry Teh\\AppData\\Local\\Temp\\ipykernel_1660\\3445851241.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
      "C:\\Users\\Terry Teh\\AppData\\Local\\Temp\\ipykernel_1660\\3445851241.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform('gamma', 0.9, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -318.9072366224961\n",
      "Episode 1, Total Reward: -585.5581007282342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Terry Teh\\AppData\\Local\\Temp\\ipykernel_1660\\1739434982.py:57: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  state_batch = torch.FloatTensor(state_batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2, Total Reward: -189.40773445014497\n",
      "Episode 3, Total Reward: -326.51467033600943\n",
      "Episode 4, Total Reward: -308.29743967922377\n",
      "Episode 5, Total Reward: -463.67328125518924\n",
      "Episode 6, Total Reward: -146.8794712703745\n",
      "Episode 7, Total Reward: -154.09308379207621\n",
      "Episode 8, Total Reward: -336.2867523165479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-11 00:18:55,418] Trial 0 finished with value: -308.19981628270676 and parameters: {'lr': 0.00790558268335284, 'gamma': 0.9509440699573725, 'batch_size': 128, 'target_update_frequency': 500}. Best is trial 0 with value: -308.19981628270676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9, Total Reward: -252.3803923767715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Terry Teh\\AppData\\Local\\Temp\\ipykernel_1660\\3445851241.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
      "C:\\Users\\Terry Teh\\AppData\\Local\\Temp\\ipykernel_1660\\3445851241.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform('gamma', 0.9, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -251.33041391185637\n",
      "Episode 1, Total Reward: -260.23173719721206\n",
      "Episode 2, Total Reward: -323.2997105792823\n",
      "Episode 3, Total Reward: -184.4195232359948\n",
      "Episode 4, Total Reward: -325.3352141425354\n",
      "Episode 5, Total Reward: -185.84395677956337\n",
      "Episode 6, Total Reward: -192.58449262000602\n",
      "Episode 7, Total Reward: -282.85551002242005\n",
      "Episode 8, Total Reward: -206.73803979741643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-11 00:18:59,550] Trial 1 finished with value: -251.79971288899037 and parameters: {'lr': 5.9018754034715956e-05, 'gamma': 0.9488550748981232, 'batch_size': 64, 'target_update_frequency': 500}. Best is trial 1 with value: -251.79971288899037.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9, Total Reward: -305.35853060361706\n",
      "Saved parameters to disk\n",
      "Episode 0, Total Reward: -219.274621806715\n",
      "Episode 1, Total Reward: -290.0454661081469\n",
      "Episode 2, Total Reward: -211.4889324724608\n",
      "Episode 3, Total Reward: -269.0099830683223\n",
      "Episode 4, Total Reward: -322.35969816703937\n",
      "Episode 5, Total Reward: -371.8429359766541\n",
      "Episode 6, Total Reward: -227.678740115424\n",
      "Episode 7, Total Reward: -253.07858670278517\n",
      "Episode 8, Total Reward: -152.0877508840339\n",
      "Episode 9, Total Reward: -282.22767313355274\n",
      "Episode 10, Total Reward: -418.1023112976398\n",
      "Episode 11, Total Reward: -249.04633805513123\n",
      "Episode 12, Total Reward: -179.12076063697714\n",
      "Episode 13, Total Reward: -183.03040211573455\n",
      "Episode 14, Total Reward: -177.52102553519498\n",
      "Episode 15, Total Reward: -303.05141322051963\n",
      "Episode 16, Total Reward: -190.80353646647467\n",
      "Episode 17, Total Reward: -189.60018678119465\n",
      "Episode 18, Total Reward: -173.71017218421332\n",
      "Episode 19, Total Reward: -162.88537901682307\n",
      "Episode 20, Total Reward: -192.7709389362043\n",
      "Episode 21, Total Reward: -331.13302336944093\n",
      "Episode 22, Total Reward: -316.8953152126681\n",
      "Episode 23, Total Reward: -117.39000738177882\n",
      "Episode 24, Total Reward: -269.39816350358115\n",
      "Episode 25, Total Reward: -317.99953275033147\n",
      "Episode 26, Total Reward: -117.94282134819055\n",
      "Episode 27, Total Reward: -192.75742797583513\n",
      "Episode 28, Total Reward: -232.3922436343335\n",
      "Episode 29, Total Reward: -353.9072992179048\n",
      "Episode 30, Total Reward: -259.15275144731237\n",
      "Episode 31, Total Reward: -164.02220651684047\n",
      "Episode 32, Total Reward: -196.63606830843162\n",
      "Episode 33, Total Reward: -294.6474136402822\n",
      "Episode 34, Total Reward: -179.06039064502562\n",
      "Episode 35, Total Reward: -121.07080320204791\n",
      "Episode 36, Total Reward: -163.59528752632826\n",
      "Episode 37, Total Reward: -261.617262278851\n",
      "Episode 38, Total Reward: -3344.659046080255\n",
      "Episode 39, Total Reward: -298.60121346225225\n",
      "Episode 40, Total Reward: -345.61914045028084\n",
      "Episode 41, Total Reward: -574.7541292076562\n",
      "Episode 42, Total Reward: -200.01149829631495\n",
      "Episode 43, Total Reward: -375.53424070642257\n",
      "Episode 44, Total Reward: -493.74614444348606\n",
      "Episode 45, Total Reward: -207.44210435997041\n",
      "Episode 46, Total Reward: -573.0108655183665\n",
      "Episode 47, Total Reward: -135.50970597284226\n",
      "Episode 48, Total Reward: -196.357746425983\n",
      "Episode 49, Total Reward: -589.426082612459\n",
      "Episode 50, Total Reward: -445.43649924885534\n",
      "Episode 51, Total Reward: -242.31348697886014\n",
      "Episode 52, Total Reward: -384.4365500350116\n",
      "Episode 53, Total Reward: -405.3616585527675\n",
      "Episode 54, Total Reward: -481.79096452973704\n",
      "Episode 55, Total Reward: -401.00036327191896\n",
      "Episode 56, Total Reward: -94.12941133218244\n",
      "Episode 57, Total Reward: -360.0877551198807\n",
      "Episode 58, Total Reward: -42.212768943479475\n",
      "Episode 59, Total Reward: -507.13082587752024\n",
      "Episode 60, Total Reward: -311.70976040794574\n",
      "Episode 61, Total Reward: -505.4893088644332\n",
      "Episode 62, Total Reward: -1.2605579721710427\n",
      "Episode 63, Total Reward: -415.05575402968844\n",
      "Episode 64, Total Reward: -72.46887523701284\n",
      "Episode 65, Total Reward: -555.4429098348025\n",
      "Episode 66, Total Reward: -387.43352264986396\n",
      "Episode 67, Total Reward: -513.9126400443777\n",
      "Episode 68, Total Reward: -457.510764160821\n",
      "Episode 69, Total Reward: -234.23706199424714\n",
      "Episode 70, Total Reward: -71.58841135132879\n",
      "Episode 71, Total Reward: -226.0876672237897\n",
      "Episode 72, Total Reward: -454.76384301049944\n",
      "Episode 73, Total Reward: -477.71117746018825\n",
      "Episode 74, Total Reward: -534.9549850814302\n",
      "Episode 75, Total Reward: -57.55339237675289\n",
      "Episode 76, Total Reward: -309.35735009926947\n",
      "Episode 77, Total Reward: -90.43514843209013\n",
      "Episode 78, Total Reward: -99.59661747455026\n",
      "Episode 79, Total Reward: -341.74291649863414\n",
      "Episode 80, Total Reward: -96.46845149737145\n",
      "Episode 81, Total Reward: -174.88180609901647\n",
      "Episode 82, Total Reward: -473.9494301120244\n",
      "Episode 83, Total Reward: -450.65761180108694\n",
      "Episode 84, Total Reward: -499.8273695081715\n",
      "Episode 85, Total Reward: -55.01161536043738\n",
      "Episode 86, Total Reward: -413.0172339351194\n",
      "Episode 87, Total Reward: -74.08887753566218\n",
      "Episode 88, Total Reward: -99.46963964739939\n",
      "Episode 89, Total Reward: -100.39272329443152\n",
      "Episode 90, Total Reward: -280.1947558322216\n",
      "Episode 91, Total Reward: -358.7694743105239\n",
      "Episode 92, Total Reward: -507.5060510131738\n",
      "Episode 93, Total Reward: -355.0185305534137\n",
      "Episode 94, Total Reward: -101.29247138930785\n",
      "Episode 95, Total Reward: -85.11887219600234\n",
      "Episode 96, Total Reward: -510.00116843168166\n",
      "Episode 97, Total Reward: -110.98973657162229\n",
      "Episode 98, Total Reward: -823.6833484598593\n",
      "Episode 99, Total Reward: -66.22169752928671\n",
      "Episode 100, Total Reward: -260.1365659738108\n",
      "Episode 101, Total Reward: -212.2170522152187\n",
      "Episode 102, Total Reward: -57.376216938337365\n",
      "Episode 103, Total Reward: -459.5481970889417\n",
      "Episode 104, Total Reward: -76.26924971969534\n",
      "Episode 105, Total Reward: -417.49184092434126\n",
      "Episode 106, Total Reward: -773.5932242197777\n",
      "Episode 107, Total Reward: -448.64193947992663\n",
      "Episode 108, Total Reward: -181.4537262435253\n",
      "Episode 109, Total Reward: -481.519542512416\n",
      "Episode 110, Total Reward: -223.70350950387913\n",
      "Episode 111, Total Reward: -18.890850764063572\n",
      "Episode 112, Total Reward: -365.0307464679459\n",
      "Episode 113, Total Reward: -316.5571565836636\n",
      "Episode 114, Total Reward: -480.0190866664276\n",
      "Episode 115, Total Reward: -322.98691475122587\n",
      "Episode 116, Total Reward: -827.3571080700624\n",
      "Episode 117, Total Reward: -690.6880736441576\n",
      "Episode 118, Total Reward: -407.5372509383606\n",
      "Episode 119, Total Reward: -478.29113037791666\n",
      "Episode 120, Total Reward: -83.40464311180747\n",
      "Episode 121, Total Reward: -569.6583834101524\n",
      "Episode 122, Total Reward: -516.5378675914026\n",
      "Episode 123, Total Reward: -1131.2752820057833\n",
      "Episode 124, Total Reward: -491.16156922087214\n",
      "Episode 125, Total Reward: -131.62270387135806\n",
      "Episode 126, Total Reward: -674.5388871612583\n",
      "Episode 127, Total Reward: -999.1044068757271\n",
      "Episode 128, Total Reward: -2984.439051795266\n",
      "Episode 129, Total Reward: -269.76217980369336\n",
      "Episode 130, Total Reward: -344.6790847857874\n",
      "Episode 131, Total Reward: -305.5132346044927\n",
      "Episode 132, Total Reward: -398.26285915353384\n",
      "Episode 133, Total Reward: -345.7213283197382\n",
      "Episode 134, Total Reward: -224.53857778835305\n",
      "Episode 135, Total Reward: -103.64922680708622\n",
      "Episode 136, Total Reward: -290.86268935215406\n",
      "Episode 137, Total Reward: -169.98174372334296\n",
      "Episode 138, Total Reward: -192.77196280715333\n",
      "Episode 139, Total Reward: -458.2905740459134\n",
      "Episode 140, Total Reward: -273.6206283641401\n",
      "Episode 141, Total Reward: -293.524655666756\n",
      "Episode 142, Total Reward: -342.23003069147137\n",
      "Episode 143, Total Reward: -2553.2488312704295\n",
      "Episode 144, Total Reward: -638.5929885924502\n",
      "Episode 145, Total Reward: -613.4392953101154\n",
      "Episode 146, Total Reward: -21.809620626062937\n",
      "Episode 147, Total Reward: -2686.437638709681\n",
      "Episode 148, Total Reward: -33.7904842085122\n",
      "Episode 149, Total Reward: -458.54575409702153\n"
     ]
    }
   ],
   "source": [
    "STEP_COUNT = 0\n",
    "optimizer = Optimizer(env, main_network, target_network, replay_buffer, \"./model/model.pth\", \"./model/params.pkl\")\n",
    "best_params = optimizer.optimize(n_trials=2, save_params=True)\n",
    "optimizer = optim.Adam(main_network.parameters(), lr=best_params['lr'])\n",
    "trainer = DQNTrainer(env, main_network, target_network, optimizer, replay_buffer, \"./model/model.pth\", gamma=best_params['gamma'], batch_size=best_params['batch_size'], target_update_frequency=best_params['target_update_frequency'])\n",
    "trainer.train(1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
